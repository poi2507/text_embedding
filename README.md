
# 한국어 임배딩 1,2장 정리

## 1.1 임배딩이란

"만약 컴퓨터가 인간을 속여 자신을 마치 인간인 것처럼 믿게 할 수 있다면 컴퓨터를 '인텔리전트' 히디고 부를 만한 가치가 충분히 있다"<br>
> 엘런 튜링

* 컴퓨터는 어디까지나 빠르고 효율적인 '계산기'일 뿐이다.<br> 
* 컴퓨터는 인간이 사용하는 자연어(natural language)를 있는 그대로 이해(x) 숫자를 계산한다. <br>

자연어 처리 분야에서 **임베딩**이란, 사람이 쓰는 자연어를 기계가 이해할 수 있는 숫자의 나열인 **벡터**로 바꾼 결과 혹은 일련의 과정을 의미한다. <br>
단어나 문장 가각을 벡터로 변환해 **벡터 공간**으로 '끼워 넣는다'는 의미에서 임베딩이라는 이름이 붙었다. <br>
<br>

## 1.2 임베딩의 역활
임베딩은 다음 역할을 수행할 수 있다.
* 단어/문장 간 관련도 계산
* 의미적/문법적 정보 함축
* **전이 학습**

### 1.2.1 단어/문장 간 관련도 계산
쿼리 단어별로 벡터간 유사도 측정 기법의 일종인 코사인 유사도 사용(Word2Vec)
<img width="1016" alt="스크린샷 2020-11-01 오후 9 39 29" src="https://user-images.githubusercontent.com/45013435/97803250-39768580-1c8c-11eb-9767-8900861a081f.png">

표에서 관찰할수 있는 것처럼 **희망**과 코사인 유사도가 가장 높은 것은 **소망** 이라는 단어다. 마찬가지로 **절망** 은 **체념** 과 관련성이 높다 자연어일 때는 불가능했던 코사인 유사도 계산이 임베딩 덕분에 가능해졌다. <br><br>
임베딩을 수행하면 벡터 공간을 기하학적으로 나타낸 시각화 역시 가능하다. 

![IMG_452B36CA3EC8-1](https://user-images.githubusercontent.com/45013435/97803397-01237700-1c8d-11eb-8be5-0f215837b07f.jpeg)

### 1.2.2 의미/문법정보 함축
임베딩은 벡터인 만큼 사친연산이 가능하다. 단어 벡터 간 덧셈/뺄셈을 통해 단어들 사이의 의미적, 문법적 관계를 도출해낼 수 있다. <br>
구체적으로 **첫 번째 단어 벡터 - 두 번째 단어벡터 + 세 번째 단어 벡터를** 계산할 수 있다. -> ex) 아들 - 딸 + 소녀 = 소년
<br>
위의 예시가 성립하면 성공적인 임베딩이라고 볼 수 있다. 이렇게 단어 임베딩을 평가하는 방법을 **단어 유추 평가(word analogy test)** 라고 부른다.  

### 1.2.3 전이 학습

임베딩은 다른 딥러닝 모델의 입력값으로 자주 쓰인다. <br>
품질이 좋은 임베딩을 쓰면 문서 분류 정확도와 학습 속도가 올라간다.
* 임베딩을 다른 딥러닝 모델의 입력값으로 쓰는 기법을 **전이 학습** 이라고 한다.
&nbsp; 전이 학습은 사람의 학습과 비슷한 점이 있다. 사람은 무언가를 배울 때 제로에서 시작하지 않는다. 평생 쌓아온 지식을 바탕으로 새로운 사실을 빠르게 이해해한다 전이 학습 모델 역시 제로(0)부터 시작하지 않는다.
대규모 말뭉치를 활용해 임베딩을 미리 만들어 놓는다. 임베딩에는 의미적, 문법적 정보 등이 녹아 있다. 이 임베딩을 입력값으로 쓰는 전이 학습 모델은 문서 분류라는 task를 빠르게 잘 할 수 있게 된다.
&nbsp; 예시로 하나의 모델을 만든다(양방향 LSTM에 어텐션 메커니즘) 이 딥러닝 모델의 입력값은 단어 임베딩 기법의 일종인 FastText임베딩(100차원)을 사용했다. FastText임베딩은 59만 건에 이르는 한국어 문서를 미리 학습했다. 이 모델의 작동 방식을 간단하게 설명하면
* 이 영화 꿀잼 + 긍정(positive)
* 이 영화 노잼 + 부정(negative)
&nbsp; 이 전이 학습 모델은 문장을 입력받으면 해당 문장이 긍정인지 부정인지를 출력한다. 첫 번째 문장의 경우 이, 영화, 꿀잼 임베딩 세개가 순차적으로 입력된다. 두 번째 문장은 이, 영화, 노잼 임베딩이 차례로 들어간다. 이후 모델은 이 입력 벡터들을 계산해 문장의 극성을 예측한다.
<br>
e(이)  = [-0.20195.....-0.0674]<br>
e(영화) = [-0.0795.....0.11161]<br>
e(꿀잼) = [-0.08885.....0.02243]<br>
e(노잼) = [-0.009156.....0.01423]<br>

모델의 학습 과정별 **단순정확도**를 나타낸 것이다. 예시로 네이버 영화 리뷰를 사용했을 경우

<img width="737" alt="스크린샷 2020-11-01 오후 10 27 57" src="https://user-images.githubusercontent.com/45013435/97804182-8446cc00-1c91-11eb-9faa-1b75376d4648.png">

파란색 선으로 나타나 FastText는 모델 입력값으로 FastText임베딩을 사용한 것이다. Random은 입력 벡터를 랜덤 초기화한 뒤 모델을 학습한 방식이다. 즉 Random은 학습을 제로부터 시작한 모델이다.<br>
FastText임베딩을 사용한 모델의 성능이 시종일관 좋다 임베딩의 품질이 좋으면 수행하려는 task의 성능 역시 올라간다. **임베딩이 중요한 이유**이다.
<br>

다음은 문서 극성 분류 모델의 학습 과정별 **학습손실** 을 그래프로 나타낸 것이다.  
<img width="745" alt="스크린샷 2020-11-01 오후 10 33 00" src="https://user-images.githubusercontent.com/45013435/97804294-38e0ed80-1c92-11eb-9ae5-47ea8f4ae050.png">



FastText임베딩을 사용한 모델의 학습 손실이 Random보다 작고 

FastText임베딩을 사용한 모델의 성능이 시종일관 좋다 임베딩의 품질이 좋으면 수행하려는 task의 성능 역시 올라간다. **임베딩이 중요한 이유**이다.











